{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User agent set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######fake useragent for import#######\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "#need regex to use chrome data from fake_user_agent\n",
    "browser_regex = re.compile('((\\w+)[\\/][^\\s]+)')\n",
    "\n",
    "#regex finds dictionary of matches, must grab first\n",
    "def custom_agent():\n",
    "    x = browser_regex.findall(ua.random)\n",
    "    browser = x[random.randrange(0,len(x))][0]\n",
    "    user_agent = {'User-agent': browser} \n",
    "    return(user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles(soup):\n",
    "    url_list = []\n",
    "    \n",
    "    for tile in soup.find_all(class_='tile Tile-oazi1d-0 bbbEOC'):\n",
    "        url_list.append('http://stockx.com'+tile.find('a')['href'])\n",
    "    return(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract URLS for most popular shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############PULL SNEAKER URLS###################\n",
    "url = 'https://stockx.com/sneakers/most-popular?page={}' \n",
    "urls = []\n",
    "\n",
    "# 40 shoes per most popular page, 14*40 = 560 shoes\n",
    "for i in range(1,14):\n",
    "    \n",
    "    \"\"\"\n",
    "    GOES TO THE 14 MOST POPULAR PAGES.\n",
    "    ALL THE SHOES HAVE SHOE INFORMATION IN TILES.\n",
    "    GRAB THE URL PER TILE.\n",
    "    \"\"\"\n",
    "    \n",
    "    # timer\n",
    "    time.sleep(30+random.random())\n",
    "    \n",
    "    # create url\n",
    "    url_page = str.format(url, i)\n",
    "    \n",
    "    # pull response and construct soup\n",
    "    session = requests.Session()\n",
    "    response  = session.get(url_page, headers = custom_agent())\n",
    "    print(response.status_code)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    # append pulled urls to list\n",
    "    urls= urls + get_tiles(soup)\n",
    "\n",
    "#check if 520\n",
    "len(urls)\n",
    "\n",
    "#save as file to save time\n",
    "with open(\"urls.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(urls, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in previously saved data\n",
    "with open(\"urls.txt\", \"rb\") as fp:   # Unpickling\n",
    "    urls = pickle.load(fp)\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(dict.fromkeys(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull functions\n",
    "for different elements of each url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_gauge_vars(soup):\n",
    "    #this requires the soup to be pulled\n",
    "    var_dict = defaultdict(str)\n",
    "    for gauge in soup.find_all(class_='gauge-container'):\n",
    "        if(gauge.find(class_='gauge-value')):\n",
    "            var_dict[gauge.find(class_='gauge-title').text] = gauge.find(class_='gauge-value').text\n",
    "        elif(gauge.find(class_='gauge-value-negative')):\n",
    "            var_dict[gauge.find(class_='gauge-title').text] = gauge.find(class_='gauge-value-negative').text\n",
    "        else:\n",
    "            print(gauge)\n",
    "    return(var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_row_list_vars(soup):\n",
    "    #this requires the soup to be pulled\n",
    "    var_dict = defaultdict(str)\n",
    "    for row_list in soup.find_all(class_='row list-unstyled'):\n",
    "        if(row_list.find(class_='ft-high-low-col')):\n",
    "            var_dict['52 Week High/Low'] =  row_list.find(class_='ft-high-low-col').text\n",
    "        if(row_list.find(class_='ds-range value-container')):\n",
    "            var_dict['12 Month Trade Range'] =  row_list.find(class_='ds-range value-container').text\n",
    "        if(row_list.find(class_='volatility-col market-down')):\n",
    "            var_dict['Volatility'] =  row_list.find(class_='volatility-col market-down').text\n",
    "    return(var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_inset_vars(soup):\n",
    "    #this requires the soup to be pulled\n",
    "    var_dict = defaultdict(str)\n",
    "    for inset in soup.find_all(class_='inset'):\n",
    "        if(inset.find(class_='title')):\n",
    "            var_dict[inset.find(class_='title').text.strip()] = inset.find(class_='subtitle').text.strip()\n",
    "        else:\n",
    "            return({'all':0})\n",
    "    return(var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_pinfo_vars(soup):\n",
    "    #this requires the soup to be pulled\n",
    "    var_dict = defaultdict(str)\n",
    "    pinfo = soup.find(class_='product-info')\n",
    "    for detail in pinfo.find_all(class_='detail'):\n",
    "        #print(detail.text)\n",
    "        \n",
    "        for span in detail.find_all('span', recursive=False):\n",
    "            #print(detail.find(class_='pinfo-container').text)\n",
    "            #print(span.text)\n",
    "            var_dict[detail.find(class_='pinfo-container').text] = span.text\n",
    "    return(var_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge dictionary function\n",
    "def Merge(dictlist): \n",
    "    return_dict = {}\n",
    "    for dict_obc in dictlist:\n",
    "        return_dict= {**return_dict, **dict_obc}\n",
    "    return(return_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull variables by extracting urls for different shoe sizes of the shoe\n",
    "def pull_vars(url):\n",
    "    tile_response  = requests.get(url, headers = custom_agent()) \n",
    "    if (tile_response.status_code != 200):\n",
    "        print(tile_response.status_code)\n",
    "        time.sleep(500)\n",
    "        try:\n",
    "            tile_response  = requests.get(url, headers = custom_agent()) \n",
    "        except:\n",
    "            return({})\n",
    "    tile_soup = BeautifulSoup(tile_response.text, \"html\")\n",
    "    dict_list = []\n",
    "    sizes = pull_inset_vars(tile_soup)\n",
    "    for key in sizes:\n",
    "        size_info = {'Size': key, 'Lowest Ask': sizes[key]}\n",
    "        if(key != 'ALL' and sizes[key] != 'BID'):\n",
    "            time.sleep(30+random.random())\n",
    "            row = Merge([pull_x_vars(url, key),size_info])\n",
    "            dict_list.append(row)\n",
    "            \n",
    "    return(dict_list)\n",
    "\n",
    "#pull last sale prices from the url\n",
    "def pull_x_vars(url, size):\n",
    "    size_url = url+'?size='+size\n",
    "    tile_response = requests.get(size_url, headers = custom_agent())\n",
    "    if (tile_response.status_code != 200):\n",
    "        print(tile_response.status_code)\n",
    "        time.sleep(500)\n",
    "        try:\n",
    "            tile_response  = requests.get(size_url, headers = custom_agent())\n",
    "        except:\n",
    "            return({\"url\":size_url})\n",
    "    \n",
    "    tile_soup = BeautifulSoup(tile_response.text, \"html\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_value = {'Sale Value' : tile_soup.find(class_='sale-value').text,\n",
    "              'Name' : tile_soup.find(class_='name').text}\n",
    "    \n",
    "    gauge = pull_gauge_vars(tile_soup)\n",
    "    pinfo = pull_pinfo_vars(tile_soup)\n",
    "    row_list = pull_row_list_vars(tile_soup)\n",
    "    url_dict = {'url' : size_url}\n",
    "    returndict = Merge([y_value,gauge,pinfo,row_list,url_dict])\n",
    "    return(returndict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual webscraping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data pull\n",
    "df_list = []\n",
    "missed_urls = []\n",
    "\n",
    "for ix,url in tqdm(enumerate(urls)):\n",
    "    try:\n",
    "        df_list.extend(pull_vars(url).copy())\n",
    "        df = pd.DataFrame(df_list)\n",
    "        df.to_pickle('shoes_temp.pkl')\n",
    "    except Exception as err: \n",
    "        print(ix)\n",
    "        missed_urls.extend(url)\n",
    "        \n",
    "df = pd.DataFrame(df_list)\n",
    "df.to_pickle('shoes.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
